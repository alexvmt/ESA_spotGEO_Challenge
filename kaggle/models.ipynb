{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0d34db8-be25-43c1-9750-300fbf4cf443",
    "_uuid": "6ed2312e-4fb4-4d4f-bb8b-259e7c1c1e33"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10f0e46b-3649-45e8-b4db-4e45f5c45186",
    "_uuid": "529e7d75-c836-487e-8e68-14fb53f0cc18"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c8b4026-470a-4d9c-93df-77de75188f0d",
    "_uuid": "dcbb33fe-c732-46d6-9d24-f5b87e810363"
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    return plt.imread(path)\n",
    "\n",
    "def read_annotation_file(path):\n",
    "    with open(path) as annotation_file:\n",
    "        annotation_list = json.load(annotation_file)\n",
    "    # Transform list of annotations into dictionary\n",
    "    annotation_dict = {}\n",
    "    for annotation in annotation_list:\n",
    "        sequence_id = annotation['sequence_id']\n",
    "        if sequence_id not in annotation_dict:\n",
    "            annotation_dict[sequence_id] = {}\n",
    "        annotation_dict[sequence_id][annotation['frame']] = annotation['object_coords']\n",
    "    return annotation_dict\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "def random_different_coordinates(coords, size_x, size_y, pad):\n",
    "    \"\"\" Returns a random set of coordinates that is different from the provided coordinates, \n",
    "    within the specified bounds.\n",
    "    The pad parameter avoids coordinates near the bounds.\"\"\"\n",
    "    good = False\n",
    "    while not good:\n",
    "        good = True\n",
    "        c1 = random.randint(pad + 1, size_x - (pad + 1))\n",
    "        c2 = random.randint(pad + 1, size_y -( pad + 1))\n",
    "        for c in coords:\n",
    "            if c1 == c[0] and c2 == c[1]:\n",
    "                good = False\n",
    "                break\n",
    "    return (c1,c2)\n",
    "\n",
    "def extract_neighborhood(x, y, arr, radius):\n",
    "    \"\"\" Returns a 1-d array of the values within a radius of the x,y coordinates given \"\"\"\n",
    "    return arr[(x - radius) : (x + radius + 1), (y - radius) : (y + radius + 1)].ravel()\n",
    "\n",
    "def check_coordinate_validity(x, y, size_x, size_y, pad):\n",
    "    \"\"\" Check if a coordinate is not too close to the image edge \"\"\"\n",
    "    return x >= pad and y >= pad and x + pad < size_x and y + pad < size_y\n",
    "\n",
    "def generate_labeled_data(image_path, annotation, nb_false, radius):\n",
    "    \"\"\" For one frame and one annotation array, returns a list of labels \n",
    "    (1 for true object and 0 for false) and the corresponding features as an array.\n",
    "    nb_false controls the number of false samples\n",
    "    radius defines the size of the sliding window (e.g. radius of 1 gives a 3x3 window)\"\"\"\n",
    "    features,labels = [],[]\n",
    "    im_array = read_image(image_path)\n",
    "    # True samples\n",
    "    for obj in annotation:\n",
    "        obj = [int(x + .5) for x in obj] #Project the floating coordinate values onto integer pixel coordinates.\n",
    "        # For some reason the order of coordinates is inverted in the annotation files\n",
    "        if check_coordinate_validity(obj[1],obj[0],im_array.shape[0],im_array.shape[1],radius):\n",
    "            features.append(extract_neighborhood(obj[1],obj[0],im_array,radius))\n",
    "            labels.append(1)\n",
    "    # False samples\n",
    "    for i in range(nb_false):\n",
    "        c = random_different_coordinates(annotation,im_array.shape[1],im_array.shape[0],radius)\n",
    "        features.append(extract_neighborhood(c[1],c[0],im_array,radius))\n",
    "        labels.append(0)\n",
    "    return np.array(labels),np.stack(features,axis=1)\n",
    "\n",
    "def generate_labeled_set(annotation_array, path, sequence_id_list, radius, nb_false):\n",
    "    # Generate labeled data for a list of sequences in a given path\n",
    "    labels,features = [],[]\n",
    "    for seq_id in sequence_id_list:\n",
    "        for frame_id in range(1,6):\n",
    "            d = generate_labeled_data(f\"{path}{seq_id}/{frame_id}.png\",\n",
    "                                    annotation_array[seq_id][frame_id],\n",
    "                                    nb_false,\n",
    "                                    radius)\n",
    "            labels.append(d[0])\n",
    "            features.append(d[1])\n",
    "    return np.concatenate(labels,axis=0), np.transpose(np.concatenate(features,axis=1))\n",
    "\n",
    "def classify_image(im, model, radius):\n",
    "    n_features=(2*radius+1)**2 #Total number of pixels in the neighborhood\n",
    "    feat_array=np.zeros((im.shape[0],im.shape[1],n_features))\n",
    "    for x in range(radius+1,im.shape[0]-(radius+1)):\n",
    "        for y in range(radius+1,im.shape[1]-(radius+1)):\n",
    "            feat_array[x,y,:]=extract_neighborhood(x,y,im,radius)\n",
    "    all_pixels=feat_array.reshape(im.shape[0]*im.shape[1],n_features)\n",
    "    pred_pixels=model.predict(all_pixels).astype(np.bool_)\n",
    "    pred_image=pred_pixels.reshape(im.shape[0],im.shape[1])\n",
    "    return pred_image\n",
    "\n",
    "def extract_centroids(pred, bg):\n",
    "    conn_comp=measure.label(pred, background=bg)\n",
    "    object_dict=defaultdict(list) #Keys are the indices of the connected components and values are arrrays of their pixel coordinates \n",
    "    for (x,y),label in np.ndenumerate(conn_comp):\n",
    "            if label != bg:\n",
    "                object_dict[label].append([x,y])\n",
    "    # Mean coordinate vector for each object, except the \"0\" label which is the background\n",
    "    centroids={label: np.mean(np.stack(coords),axis=0) for label,coords in object_dict.items()}\n",
    "    object_sizes={label: len(coords) for label,coords in object_dict.items()}\n",
    "    return centroids, object_sizes\n",
    "\n",
    "def filter_large_objects(centroids,object_sizes, max_size):\n",
    "    small_centroids={}\n",
    "    for label,coords in centroids.items():\n",
    "            if object_sizes[label] <= max_size:\n",
    "                small_centroids[label]=coords\n",
    "    return small_centroids\n",
    "\n",
    "def predict_objects(sequence_id, frame_id, model, radius, max_size):\n",
    "    test_image = plt.imread(f\"../input/spotgeo/test/test/{sequence_id}/{frame_id}.png\")\n",
    "    test_pred=classify_image(test_image, model, radius)\n",
    "    test_centroids, test_sizes = extract_centroids(test_pred, 0)\n",
    "    test_centroids = filter_large_objects(test_centroids, test_sizes, max_size)\n",
    "    # Switch x and y coordinates for submission\n",
    "    if len(test_centroids.values()) > 0:\n",
    "        sub=np.concatenate([c[np.array([1,0])].reshape((1,2)) for c in test_centroids.values()])\n",
    "        #np array converted to list for json seralization, truncated to the first 30 elements\n",
    "        return sub.tolist()[0:30]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "824b521f-2337-4031-87fa-c9b55a3bb342",
    "_uuid": "49fbd5cd-e05c-4b5a-b04f-d5735c33891f"
   },
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d408e6cf-8928-4505-99c3-58d28900d9b5",
    "_uuid": "80c8bfe9-6aa5-49c8-9287-f571d16ad0c7"
   },
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6bf2932a-c82f-4ccb-9ff4-7890d52d7c56",
    "_uuid": "47537c61-f53c-44f4-b809-5fb2d26743d9"
   },
   "outputs": [],
   "source": [
    "train_annotation = read_annotation_file('../input/spotgeo/train_anno.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b41dc0c3-f24f-488e-84e4-5df30b1828cc",
    "_uuid": "1c50efac-8ddf-4dcb-aa92-79b110ea0f4a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "radius = 3\n",
    "train_labels, train_features = generate_labeled_set(train_annotation, '../input/spotgeo/train/train/', range(1,1001), radius, 10)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83d9851c-2ad1-45fa-841a-a391c4d1d5cc",
    "_uuid": "32cf7a6d-0a60-4a1a-b0ba-d552b84a6419"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "validation_labels, validation_features = generate_labeled_set(train_annotation, '../input/spotgeo/train/train/', range(1001,1281), radius, 10)\n",
    "\n",
    "print(validation_labels.shape)\n",
    "print(validation_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state = random_state, n_jobs = -1)\n",
    "\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [10,20,50,100,250,500,750,1000]\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=10, cv=3, verbose=2, random_state=random_state, n_jobs=-1, scoring='f1')\n",
    "\n",
    "rf_random.fit(train_features[:10000], train_labels[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = rf_random.best_params_['n_estimators'],\n",
    "                               max_features = rf_random.best_params_['max_features'],\n",
    "                               max_depth = rf_random.best_params_['max_depth'],\n",
    "                               min_samples_split = rf_random.best_params_['min_samples_split'],\n",
    "                               min_samples_leaf = rf_random.best_params_['min_samples_leaf'],\n",
    "                               bootstrap = rf_random.best_params_['bootstrap'],\n",
    "                               n_jobs = -1,\n",
    "                               random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('/kaggle/working/rf.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sub_list=predict_objects(1,1,model,radius,1)\n",
    "print(sub_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "981fd2e0-d27e-4cc4-a705-2e4e78e0742d",
    "_uuid": "727a2e93-5550-4715-95a5-56a330fc37b1"
   },
   "source": [
    "## train xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5922f97d-6fe7-45aa-a54c-86ddbb5d4f55",
    "_uuid": "426a8582-db51-4bc1-bbac-309c60548701"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(random_state = random_state, n_jobs = -1)\n",
    "\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [10,20,50,100,250,500,750,1000]\n",
    "\n",
    "learning_rate = [0.1,0.05,0.01,0.005,0.001]\n",
    "\n",
    "max_depth = [3,4,5,6,7]\n",
    "\n",
    "colsample_bytree = [0.7,0.8,0.9,1]\n",
    "\n",
    "subsample = [0.7,0.8,0.9,1]\n",
    "\n",
    "gamma = [0,1,5]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_depth': max_depth,\n",
    "               'colsample_bytree': colsample_bytree,\n",
    "               'subsample': subsample,\n",
    "               'gamma': gamma}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb = xgb.XGBClassifier()\n",
    "\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2, random_state = random_state, n_jobs = -1, scoring = 'f1')\n",
    "\n",
    "xgb_random.fit(train_features[:10000], train_labels[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators = xgb_random.best_params_['n_estimators'],\n",
    "                          learning_rate = xgb_random.best_params_['learning_rate'],\n",
    "                          max_depth = xgb_random.best_params_['max_depth'],\n",
    "                          colsample_bytree = xgb_random.best_params_['colsample_bytree'],\n",
    "                          subsample = xgb_random.best_params_['subsample'],\n",
    "                          gamma = xgb_random.best_params_['gamma'],\n",
    "                          n_jobs = -1,\n",
    "                          random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('/kaggle/working/xgb.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sub_list=predict_objects(1,1,model,radius,1)\n",
    "print(sub_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train 5-layer neural net (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "dim = train_features.shape[1]\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(dim,dim,dim,dim,dim), activation = 'relu', solver='adam', random_state=random_state)\n",
    "\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('/kaggle/working/nn5.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b505c52f-dae4-46fa-aad0-62fa778f8494",
    "_uuid": "c9c9dda6-9325-4d0b-9ce1-5728d2a8322c"
   },
   "outputs": [],
   "source": [
    "pred_labels = model.predict(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b26307b-c61a-4efa-9d18-2bc34346dc91",
    "_uuid": "2e11a612-755c-40ca-a32e-eb2438aef459"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sub_list=predict_objects(1,1,model,radius,1)\n",
    "print(sub_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train 5-layer neural net (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "dim = train_features.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(dim, input_dim=dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(dim, activation='relu'))\n",
    "model.add(Dropout(0.2, seed=random_state))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(train_features, train_labels, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/nn5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['f1'])\n",
    "plt.title('F1')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict_classes(validation_features)\n",
    "\n",
    "print(classification_report(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(confusion_matrix(pred_labels, validation_labels))\n",
    "print('\\n')\n",
    "print(\"Kappa =\", cohen_kappa_score(pred_labels, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(im, model, radius):\n",
    "    n_features=(2*radius+1)**2 #Total number of pixels in the neighborhood\n",
    "    feat_array=np.zeros((im.shape[0],im.shape[1],n_features))\n",
    "    for x in range(radius+1,im.shape[0]-(radius+1)):\n",
    "        for y in range(radius+1,im.shape[1]-(radius+1)):\n",
    "            feat_array[x,y,:]=extract_neighborhood(x,y,im,radius)\n",
    "    all_pixels=feat_array.reshape(im.shape[0]*im.shape[1],n_features)\n",
    "    pred_pixels=model.predict_classes(all_pixels).astype(np.bool_)\n",
    "    pred_image=pred_pixels.reshape(im.shape[0],im.shape[1])\n",
    "    return pred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sub_list=predict_objects(1,1,model,radius,1)\n",
    "print(sub_list[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
